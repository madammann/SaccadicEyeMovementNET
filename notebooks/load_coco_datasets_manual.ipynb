{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "load_coco_datasets_manually.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install the dataset\n"
      ],
      "metadata": {
        "id": "a6twaCXVI7KH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install CocoDataset==0.1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkXyQ264I-6B",
        "outputId": "1d5761ce-5a9d-47b3-93f8-44252f6da0af"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: CocoDataset==0.1.2 in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from CocoDataset==0.1.2) (2.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pycocotools->CocoDataset==0.1.2) (1.21.5)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->CocoDataset==0.1.2) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools->CocoDataset==0.1.2) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we build the data with https://www.tensorflow.org/datasets/api_docs/python/tfds/folder_dataset/ImageFolder, \n",
        "\n",
        "tfds.folder_dataset.ImageFolder, needs to have an specific stracture. (check the documentation)\n",
        "\n",
        "\n",
        "Now, we create the stracture, but befire we need to mount the drive from left side panel-> files -> click mount drive"
      ],
      "metadata": {
        "id": "uuHTUhKUdK_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfsH5qWwObBF",
        "outputId": "6942006f-0d5a-44b1-abe3-3840979d4050"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/coco_data/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "mount the drive and select the directory\n",
        "\n",
        "first, we set the training directory"
      ],
      "metadata": {
        "id": "G_G-gK6TQpDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/coco_data/train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "735n4Xq6PrPJ",
        "outputId": "f47cb9ac-f0e5-406e-9f56-6c13f519230d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/coco_data/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the annotations"
      ],
      "metadata": {
        "id": "TK6ClLjhJWdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip /content/drive/MyDrive/coco_data/test/annotations_trainval2014.zip\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuLsPba0JYth",
        "outputId": "3a602e1e-0e06-4f15-89c4-6c7a0a1e52b2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-13 21:19:05--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.86.20\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.86.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2014.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  33.5MB/s    in 7.0s    \n",
            "\n",
            "2022-03-13 21:19:12 (34.6 MB/s) - ‘annotations_trainval2014.zip’ saved [252872794/252872794]\n",
            "\n",
            "Archive:  /content/drive/MyDrive/coco_data/test/annotations_trainval2014.zip\n",
            "  inflating: annotations/instances_train2014.json  \n",
            "  inflating: annotations/instances_val2014.json  \n",
            "  inflating: annotations/person_keypoints_train2014.json  \n",
            "  inflating: annotations/person_keypoints_val2014.json  \n",
            "  inflating: annotations/captions_train2014.json  \n",
            "  inflating: annotations/captions_val2014.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "class names for downloading\n"
      ],
      "metadata": {
        "id": "nqG9j0Y2JkZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['airplane', \"apple\", \"banana\", \"boat\", \"bus\", \"car\",\"person\",\"bicycle\",\"bird\",\"book\"]"
      ],
      "metadata": {
        "id": "o5aVmacEJjgE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "write the function to download the class with annotations and save it to content folder"
      ],
      "metadata": {
        "id": "CgDdafVuKaNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from coco_dataset import coco_dataset_download as cocod\n",
        "\n",
        "\n",
        "def manual_download(names):\n",
        "  class_name=names  #class name example \n",
        "  images_count=15       #count of images  \n",
        "  annotations_path='/content/drive/MyDrive/coco_data/test/annotations/instances_train2014.json' #path of coco dataset annotations \n",
        "  #call download function \n",
        "  cocod.coco_dataset_download(class_name,images_count,annotations_path)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bFPPoft8KZU1"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in classes:\n",
        "  manual_download(i)\n",
        "  "
      ],
      "metadata": {
        "id": "oT7_1AF7LCaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for removing the annotations\n",
        "!rm -rf annotations"
      ],
      "metadata": {
        "id": "6U7q_HUFbCMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set the test directory and download the daata these"
      ],
      "metadata": {
        "id": "Pi1MqBPxa-N6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/coco_data/test"
      ],
      "metadata": {
        "id": "5GpdzaiTa9d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip /content/drive/MyDrive/coco_data/test/annotations_trainval2014.zip\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LLLKppTWa3iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from coco_dataset import coco_dataset_download as cocod\n",
        "\n",
        "\n",
        "def manual_download(names):\n",
        "  class_name=names  #class name example \n",
        "  images_count=15       #count of images  \n",
        "  annotations_path='/content/drive/MyDrive/coco_data/test/annotations/instances_val2014.json' #path of coco dataset annotations \n",
        "  #call download function \n",
        "  cocod.coco_dataset_download(class_name,images_count,annotations_path)\n"
      ],
      "metadata": {
        "id": "XVBkZ5z5bLCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in classes:\n",
        "  manual_download(i)"
      ],
      "metadata": {
        "id": "PRorjyG9bLEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for removing the annotations\n",
        "!rm -rf annotations"
      ],
      "metadata": {
        "id": "qDpQWBKgcj8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we build the data from folder as tensors\n"
      ],
      "metadata": {
        "id": "YMz3iNQaMenx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "builder = tfds.ImageFolder('/content/drive/MyDrive/coco_data/')\n",
        "print(builder.info)  # num examples, labels... are automatically calculated\n",
        "dataset = builder.as_dataset(shuffle_files=True, as_supervised = True)\n",
        "train_ds, test_ds = dataset[\"train\"], dataset[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wilDs_EiKirG",
        "outputId": "695d3544-eb62-49ef-a2ca-acb22b6339d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='image_folder',\n",
            "    version=1.0.0,\n",
            "    description='Generic image classification dataset.',\n",
            "    homepage='https://www.tensorflow.org/datasets/catalog/image_folder',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "        'image/filename': Text(shape=(), dtype=tf.string),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=11),\n",
            "    }),\n",
            "    total_num_examples=1170,\n",
            "    splits={\n",
            "        'test': 160,\n",
            "        'train': 1010,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "to preprocess the data pipeline"
      ],
      "metadata": {
        "id": "taBRdtuTc2lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def prepare_coco_data(mnist):\n",
        "  #flatten the images into vectors\n",
        "  mnist = mnist.map(lambda img, target: (tf.image.resize(img, [64,64],\n",
        "                                         method = tf.image.ResizeMethod.BILINEAR, \n",
        "                                         preserve_aspect_ratio=False),      target))\n",
        "  \n",
        "  #convert data from uint8 to float32\n",
        "  mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))\n",
        "  #sloppy input normalization, just bringing image values from range [0, 255] to [-1, 1]\n",
        "  mnist = mnist.map(lambda img, target: ((img/128.)-1., target))\n",
        "  #create one-hot targets\n",
        "  mnist = mnist.map(lambda img, target: (img, tf.one_hot(target, depth=10)))\n",
        "  #cache this progress in memory, as there is no need to redo it; it is deterministic after all\n",
        "  mnist = mnist.cache()\n",
        "  #shuffle, batch, prefetch\n",
        "  mnist = mnist.shuffle(1000)\n",
        "  mnist = mnist.batch(32)\n",
        "  mnist = mnist.prefetch(20)\n",
        "  #return preprocessed dataset\n",
        "  return mnist\n",
        "\n",
        "train_dataset = train_ds.apply(prepare_coco_data)\n",
        "test_dataset = test_ds.apply(prepare_coco_data)"
      ],
      "metadata": {
        "id": "4kCdl0hlLPtA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "build the model(demo, just to test the data)"
      ],
      "metadata": {
        "id": "TK2J442Ac1gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__() # input 128 x 128 x 3\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(filters=36, kernel_size=(2,2), strides=(2, 2),activation=tf.nn.relu) # 64x 64x 36\n",
        "        self.max_pool = tf.keras.layers.MaxPool2D(pool_size=(2,2) ,strides=(2, 2))# 32 x 32 x 36\n",
        "        # self.dense1 = tf.keras.layers.conv2D(filters=36, kernel_size=(2,2), strides=(2, 2), padding=\"valid\",activation=tf.nn.relu)\n",
        "        self.conv_2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1, 1),activation=tf.nn.relu) # 30 x 30 x 16\n",
        "\n",
        "        self.flatten = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.out = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "        \n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        x = self.conv_1(inputs)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.conv_2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.out(x)\n",
        "        return x\n",
        "    "
      ],
      "metadata": {
        "id": "qcHhBzgbLPwB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train steps, takem from complete model train file"
      ],
      "metadata": {
        "id": "BAmS9Dxyc8Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, input, target, loss_function, optimizer):\n",
        "  # loss_object and optimizer_object are instances of respective tensorflow classes\n",
        "  with tf.GradientTape() as tape:\n",
        "    prediction = model(input)\n",
        "    loss = loss_function(target, prediction)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return loss\n",
        "\n",
        "def test(model, test_data, loss_function):\n",
        "  # test over complete test data\n",
        "\n",
        "  test_accuracy_aggregator = []\n",
        "  test_loss_aggregator = []\n",
        "\n",
        "  for (input, target) in test_data:\n",
        "    prediction = model(input)\n",
        "    sample_test_loss = loss_function(target, prediction)\n",
        "    sample_test_accuracy =  np.argmax(target, axis=1) == np.argmax(prediction, axis=1)\n",
        "    sample_test_accuracy = np.mean(sample_test_accuracy)\n",
        "    test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "    test_accuracy_aggregator.append(np.mean(sample_test_accuracy))\n",
        "\n",
        "  test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "  test_accuracy = tf.reduce_mean(test_accuracy_aggregator)\n",
        "\n",
        "  return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "zKLJyE2vLPxu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#For showcasing we only use a subset of the training and test data (generally use all of the available data!)\n",
        "train_dataset = train_dataset.take(100)\n",
        "test_dataset = test_dataset.take(10)\n",
        "\n",
        "### Hyperparameters\n",
        "num_epochs = 50\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Initialize the model.\n",
        "model = MyModel()\n",
        "# Initialize the loss: categorical cross entropy. Check out 'tf.keras.losses'.\n",
        "cross_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "# Initialize the optimizer: SGD with default parameters. Check out 'tf.keras.optimizers'\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "# Initialize lists for later visualization.\n",
        "train_losses = []\n",
        "\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "#testing once before we begin\n",
        "test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "test_losses.append(test_loss)\n",
        "test_accuracies.append(test_accuracy)\n",
        "\n",
        "#check how model performs on train data once before we begin\n",
        "train_loss, _ = test(model, train_dataset, cross_entropy_loss)\n",
        "train_losses.append(train_loss)\n",
        "\n",
        "# We train for num_epochs epochs.\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch: {str(epoch)} starting with accuracy {test_accuracies[-1]}')\n",
        "\n",
        "    #training (and checking in with training)\n",
        "    epoch_loss_agg = []\n",
        "    for input,target in train_dataset:\n",
        "        train_loss = train_step(model, input, target, cross_entropy_loss, optimizer)\n",
        "        epoch_loss_agg.append(train_loss)\n",
        "    \n",
        "    #track training loss\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss_agg))\n",
        "\n",
        "    #testing, so we can track accuracy and test loss\n",
        "    test_loss, test_accuracy = test(model, test_dataset, cross_entropy_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracies.append(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSyXVojZLPzq",
        "outputId": "4de5c037-4102-467e-9232-584e2cab7305"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 starting with accuracy 0.11875\n",
            "Epoch: 1 starting with accuracy 0.11875\n",
            "Epoch: 2 starting with accuracy 0.11875\n",
            "Epoch: 3 starting with accuracy 0.11875\n",
            "Epoch: 4 starting with accuracy 0.1125\n",
            "Epoch: 5 starting with accuracy 0.10625\n",
            "Epoch: 6 starting with accuracy 0.10625\n",
            "Epoch: 7 starting with accuracy 0.10625\n",
            "Epoch: 8 starting with accuracy 0.10625\n",
            "Epoch: 9 starting with accuracy 0.10625\n",
            "Epoch: 10 starting with accuracy 0.11875\n",
            "Epoch: 11 starting with accuracy 0.11875\n",
            "Epoch: 12 starting with accuracy 0.11875\n",
            "Epoch: 13 starting with accuracy 0.11875\n",
            "Epoch: 14 starting with accuracy 0.125\n",
            "Epoch: 15 starting with accuracy 0.11875\n",
            "Epoch: 16 starting with accuracy 0.11875\n",
            "Epoch: 17 starting with accuracy 0.11875\n",
            "Epoch: 18 starting with accuracy 0.1125\n",
            "Epoch: 19 starting with accuracy 0.1125\n",
            "Epoch: 20 starting with accuracy 0.1125\n",
            "Epoch: 21 starting with accuracy 0.1125\n",
            "Epoch: 22 starting with accuracy 0.1125\n",
            "Epoch: 23 starting with accuracy 0.1125\n",
            "Epoch: 24 starting with accuracy 0.1125\n",
            "Epoch: 25 starting with accuracy 0.1125\n",
            "Epoch: 26 starting with accuracy 0.1125\n",
            "Epoch: 27 starting with accuracy 0.10625\n",
            "Epoch: 28 starting with accuracy 0.10625\n",
            "Epoch: 29 starting with accuracy 0.10625\n",
            "Epoch: 30 starting with accuracy 0.10625\n",
            "Epoch: 31 starting with accuracy 0.10625\n",
            "Epoch: 32 starting with accuracy 0.1\n",
            "Epoch: 33 starting with accuracy 0.09375\n",
            "Epoch: 34 starting with accuracy 0.09375\n",
            "Epoch: 35 starting with accuracy 0.1\n",
            "Epoch: 36 starting with accuracy 0.1\n",
            "Epoch: 37 starting with accuracy 0.1\n",
            "Epoch: 38 starting with accuracy 0.1\n",
            "Epoch: 39 starting with accuracy 0.1\n",
            "Epoch: 40 starting with accuracy 0.1\n",
            "Epoch: 41 starting with accuracy 0.10625\n",
            "Epoch: 42 starting with accuracy 0.1125\n",
            "Epoch: 43 starting with accuracy 0.1125\n",
            "Epoch: 44 starting with accuracy 0.1125\n",
            "Epoch: 45 starting with accuracy 0.1125\n",
            "Epoch: 46 starting with accuracy 0.1125\n",
            "Epoch: 47 starting with accuracy 0.1125\n",
            "Epoch: 48 starting with accuracy 0.1125\n",
            "Epoch: 49 starting with accuracy 0.1125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize accuracy and loss for training and test data.\n",
        "plt.figure()\n",
        "line1, = plt.plot(train_losses)\n",
        "line2, = plt.plot(test_losses)\n",
        "line3, = plt.plot(test_accuracies)\n",
        "plt.xlabel(\"Training steps\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend((line1,line2, line3),(\"training\",\"test\", \"test accuracy\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "VoX7R4ZiXHtT",
        "outputId": "5ceed305-17d2-49fa-df06-2c0175586537"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZG-8KrHmXHwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}